\chapter{Theory}
\section{Theories}
One of the theories which we have tried to bring into practice, is the Vector Space Model. This is an algebraic model, that serves for representing documents as vectors of index terms. It is widely used in information retrieval, indexing and ranking of the retrieved objects according to the query-relevance. The procedure that this model employs, is divided into three parts:
\begin{enumerate}
\item{Document indexing is the process of extracting the content bearing terms.}
\item{Weighting of the indexed terms is assigning the importance to each term by a frequency metric.}
\item{Ranking the documents with respect to the query, according to a similarity metric.}
\end{enumerate}
In this model,both documents and queries are represented as vectors, in the form:
\[
d_j = (w_{1,j}, w_{2,j}, ..., w_{n,j})
\]
\[
q = (w_{1,q}, w_{2,q}, ..., w_{t,q})
\]
where each dimension corresponds to a separate term. The notion of a term, varies with the application. It can be a word, or a phrase or sentence, in our case we regard them as words. The value of the terms that do occur in the document, is non-zero. This value is normally weighted by some frequency measures. The most used one, is the tf*idf , which stands for : term-frequency* inverse- document- frequency.  This measure, tells us how important a word is, both in the context of a document, and the collection of documents where this belongs. The tf*idf value, increases proportionally with the number of times the word occurs in the document, but it is offset by the number of times the word appears in the rest of the documents. We do this, in order to avoid the counting of generally heavily used words,which are common for the whole document set, as terms that are used specifically in a particular document. 
When indexing a document, we try to remove the words that do not describe the content. These are common functional words like : the, is etc.  This indexing can be based on the term frequency, by calculating the overall frequency of a particular word. These are also called, stopwords. 
Term -Weighting  has been explained by exploring the exhaustivity and specificity, where exhaustivity is related to recall, and specificity to precision. 
The final part of this model, is concerned with the ranking of the documents, by their relevance to the query. This ranking is done by using a similarity measure called cosine similarity. As we have also pointed out in the above text, the documents and queries are represented as vectors. With the cosine similarity, we try to find out, how different these two vectors are by calculating the angle between them. The basic formula used, is:
\[
\cos(\theta) = \frac{d_2 \times q}{\parallel d_2 \parallel \times \parallel q \parallel}
\]
where the enumerator denotes the dot (vector) product, and the denominator does the normalization. As all values of the vectors are non-negative, a cos value of zero, would mean orthogonality between the vectors and thus no match between them. 
Some of the limitations that this model imposes, are:
\begin{itemize}
\item{Long documents are poorly represented, by a small scalar product and a large dimensionality.}
\item{Not every relevant words gets matched. Substrings would give a “false positive”.}
\item{There is a semantic sensitivity because of the existence of synonymy and polysemy, which hide the true semantics of the text.}
\item{When indexing the documents, the order of terms is lost.}
\item{With VSM, we are working under the assumption of statistical independence between words.}
\end{itemize}

\section{Adaption of theories}
In order to employ the conveniences of the VSM model, we had to do a bit of preparatory work.
Given the Patient-Cases, as a text file, after having read it, we had to remove the stopwords in each of the cases, in order to enlarge the scope of the actually relevant words.
Then we used the snowball-stemmer, in order to stem all the words contained in the files. Now, the files are ready to be used as a search query, with respect to the ICD/ATC.
After that, we preprocessed the Therapy Chapters (Teorikapitlene) of the NHL ,by stripping them from the HTML-tags, and transforming the file into an XML file. We performed the 'snowball'-stemming also on the therapy chapters.
The ICD10 codes were handed as an .owl file. We did the filtering by setting rules about loading the ICD10/ATC and created classes for labels codes(ICD10/ATC) and synonyms.
Then we used the 'apache.Digeseter' for each ICD10 code . After that, we put all the ICD10 classes in a list, and indexed them with the help of 'Apache.lucene'; after that, we put the already indexed files in a folder. Now we can use also the NHL- theory chapters as search parameters, when searching through the saved indexed files. In the end, the goal is to compare these two results, in order to find a relevant match between the Clinical/Patient cases and the therapy chapter of the NHL. 
As it has previously been mentioned, we employed the VSM model in order to complete the tasks. We followed, quite accurately, the steps presented in the theory, with the exception of adding a document-term matrix, something which is not covered by the VSM model theory. Listing the steps of the procedure; firstly we:
\begin{itemize}
\item{built the term-doc matrix from preprocessed therapy chapters,}
\item{then we weighted the words in each document, by using the tf*idf  formula,}
\item{followed by calculating the cosine similarity, expressed in the formula above}
\item{and finally ranged the documents in a descending similarity-metric order.}
\end{itemize}
Here follow some remarks to be noted: \\
Since only terms in the search domain were relevant for the search, we did not include unique terms from the casefiles (our queries). Then the terms were weighted using the tf*idf-method explained above. 
Another thing to be mentioned is that, we considered as separate entities, only sub-chapters that had their own html-file. The others were considered as part of the chapter. 
After a prototypical - trial of the LSI method, we found out that pure VSM was already giving results that were exactly as acceptable. 